{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f78dd507ab4f0cb7e131503d31a50a3d5d3a5e3dcaa7637587dcf584d77e2d93"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "source": [
    "# SSL for emotion analysis\n",
    "#### 25.11.2020\n",
    "\n",
    "## Proposal plan\n",
    "\n",
    "- [x] 1.Setting up the environment\n",
    "- [x] 2.Corpora preprocessing\n",
    "- [x] 3.Unlabeled data collecting & cleaning\n",
    "- [x] 4.Baseline model\n",
    "- [ ] 5.Semi-supervised Sequence Learning (SSL Model 1) [https://arxiv.org/abs/1511.01432] (Poor performance)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dataset Preparation\n",
    "- Tokenize by spaCy library and removed NLTK's stopwords\n",
    "- Made statistics and generate vocabularies for tokens have dominate class trendency. (Threshold at 40%, 60%ï¼Œ80% for top two classes)\n",
    "\n",
    "For SSEC:\n",
    "- Removed @usernames and URLs.\n",
    "- Replaced emojis like :) to special marks and added specail cases to tokenizer.\n",
    "- Used the labels with highest possible agreement, but without losing labels.\n",
    "\n",
    "For TEC:\n",
    "- Cleaned Twitter things just same as in SSEC.\n",
    "- Removed non-English samples.\n",
    "- (Optinal) Data augmentation with textaugment library, deal with the unbalance of classes.\n",
    "\n",
    "For ISEAR:\n",
    "- Handles special samples in the dataset e.g. \\[No response.\\]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TEC sample:\n['greatest', 'dancer', 'life', 'practice', 'daily', 'positive', 'habits', 'fun', 'freedom', 'habits'] 1\n"
     ]
    }
   ],
   "source": [
    "import pre.load_data as load_data\n",
    "tec_data, isear_data, ssec_data = load_data.load_data()\n",
    "print('TEC sample:')\n",
    "print(tec_data.get_data()[1], tec_data.get_target()[1])"
   ]
  },
  {
   "source": [
    "## Unlabeled Data\n",
    "\n",
    "### 1.Twitter Data\n",
    "Take a similar approach to TEC. \n",
    "Crawled 10000 tweets that have hashtags with emotions.\n",
    "Filtered out non-English tweets, about 6000-8000 left for each class.\n",
    "Then clean them up using the preceding Tokenizer.\n",
    "\n",
    "Can be used by TEC and SSEC.\n",
    "\n",
    "### 2.Story Data\n",
    "Crawled stories form [https://sayitforward.org/stories/].\n",
    "802 Stories, 34151 sentences. \n",
    "\n",
    "Can be used by ISEAR.\n",
    "\n",
    "### 3.EmoEvent (Not really unlabeled)\n",
    "From paper:\n",
    "\n",
    "EmoEvent: A Multilingual Emotion Corpus based on different Events [https://www.aclweb.org/anthology/2020.lrec-1.186/]\n",
    "\n",
    "The dataset has 7303 English tweets that based on different events and have emotions.\n",
    "\n",
    "Can be used by SSEC, TEC and ISEAR.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Baseline Model\n",
    "\n",
    "Baseline model are LSTM models, consist of:\n",
    "- Fasttext embedding matrix\n",
    "- Bidirectional LSTM layer\n",
    "- 2 Dense layers\n",
    "- Softmax for ISEAR and TEC, Sigmoid for SSEC\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SSL Model\n",
    "SSL model consist of a LSTM Language model and a LSTM classifier.\n",
    "\n",
    "The classifier has the same architecture as the baseline.\n",
    "\n",
    "The language model has a embedding layer and a LSTM layer, followed by a dense layer as decoder.\n",
    "The size of embedding matrix and LSTM weight of classifier and LM are same.\n",
    "\n",
    "We firstly train a LM from unlabeled+labeled dataset, then load the weight into classifier.\n",
    "Finally train the classifier normally.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Issues in the models\n",
    "\n",
    "## 1. Small datasets and overfitting\n",
    "\n",
    "## 2. SSL does not work well in emotion corpora\n",
    "\n",
    "### a. Crawled unlabeled data low quality\n",
    "\n",
    "### b. Datasets itself low quality\n",
    "\n",
    "## 3. How to deal with non emotional unlabeled data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}